// Copyright 2025 Advanced Micro Devices, Inc.
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

// RUN: %test_exe | iree-opt --verify-roundtrip
// RUN: %test_exe | filecheck %s --check-prefix=TORCH-CHECK
// RUN: %test_exe | iree-compile - --compile-to=input | \
// RUN:             filecheck %s --check-prefix=LINALG-CHECK

#include <fusilli.h>

#include <cassert>
#include <iostream>
#include <memory>

using namespace fusilli;

int main() {
  int64_t n = 16, c = 128, h = 64, w = 32, k = 256, r = 1, s = 1;
  auto graph = std::make_shared<Graph>();
  graph->setName("conv_asm_emitter_x_nchw_w_kcrs");
  graph->setIODataType(DataType::Float).setComputeDataType(DataType::Float);

  auto X = graph->tensor(TensorAttr()
                             .setName("arg0_image")
                             .setDim({n, c, h, w})
                             .setStride({c * h * w, h * w, w, 1})); // NCHW

  auto W = graph->tensor(TensorAttr()
                             .setName("arg1_filter")
                             .setDim({k, c, r, s})
                             .setStride({c * r * s, r * s, s, 1})); // KCRS

  auto conv_attr = ConvFPropAttr()
                       .setPadding({0, 0})
                       .setStride({1, 1})
                       .setDilation({1, 1})
                       .setName("conv_fprop");

  auto Y = graph->convFProp(X, W, conv_attr);

  Y->setName("result").setOutput(true);

  // clang-format off
  //
  // TORCH-CHECK:   module @module {
  // TORCH-CHECK:     func.func @main(%result_: !torch.tensor<[16,256,64,32],f32>, %arg0_image: !torch.vtensor<[16,128,64,32],f32>, %arg1_filter: !torch.vtensor<[256,128,1,1],f32>) attributes {torch.assume_strict_symbolic_shapes} {
  // TORCH-CHECK:       %bias_conv_fprop = torch.constant.none
  // TORCH-CHECK:       %transposed_conv_fprop = torch.constant.bool false
  // TORCH-CHECK:       %output_padding_conv_fprop = torch.prim.ListConstruct  : () -> !torch.list<int>
  // TORCH-CHECK:       %groups_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %stride_val_0_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %stride_val_1_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %stride_conv_fprop = torch.prim.ListConstruct %stride_val_0_conv_fprop, %stride_val_1_conv_fprop : (!torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %padding_val_0_conv_fprop = torch.constant.int 0
  // TORCH-CHECK:       %padding_val_1_conv_fprop = torch.constant.int 0
  // TORCH-CHECK:       %padding_conv_fprop = torch.prim.ListConstruct %padding_val_0_conv_fprop, %padding_val_1_conv_fprop : (!torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %dilation_val_0_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %dilation_val_1_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %dilation_conv_fprop = torch.prim.ListConstruct %dilation_val_0_conv_fprop, %dilation_val_1_conv_fprop : (!torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %permute_X_val_0_conv_fprop = torch.constant.int 0
  // TORCH-CHECK:       %permute_X_val_1_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %permute_X_val_2_conv_fprop = torch.constant.int 2
  // TORCH-CHECK:       %permute_X_val_3_conv_fprop = torch.constant.int 3
  // TORCH-CHECK:       %permute_X_conv_fprop = torch.prim.ListConstruct %permute_X_val_0_conv_fprop, %permute_X_val_1_conv_fprop, %permute_X_val_2_conv_fprop, %permute_X_val_3_conv_fprop : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %arg0_image_perm = torch.aten.permute %arg0_image, %permute_X_conv_fprop : !torch.vtensor<[16,128,64,32],f32>, !torch.list<int> -> !torch.vtensor<[16,128,64,32],f32>
  // TORCH-CHECK:       %permute_W_val_0_conv_fprop = torch.constant.int 0
  // TORCH-CHECK:       %permute_W_val_1_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %permute_W_val_2_conv_fprop = torch.constant.int 2
  // TORCH-CHECK:       %permute_W_val_3_conv_fprop = torch.constant.int 3
  // TORCH-CHECK:       %permute_W_conv_fprop = torch.prim.ListConstruct %permute_W_val_0_conv_fprop, %permute_W_val_1_conv_fprop, %permute_W_val_2_conv_fprop, %permute_W_val_3_conv_fprop : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %arg1_filter_perm = torch.aten.permute %arg1_filter, %permute_W_conv_fprop : !torch.vtensor<[256,128,1,1],f32>, !torch.list<int> -> !torch.vtensor<[256,128,1,1],f32>
  // TORCH-CHECK:       %result_perm = torch.aten.convolution %arg0_image_perm, %arg1_filter_perm, %bias_conv_fprop, %stride_conv_fprop, %padding_conv_fprop, %dilation_conv_fprop, %transposed_conv_fprop, %output_padding_conv_fprop, %groups_conv_fprop : !torch.vtensor<[16,128,64,32],f32>, !torch.vtensor<[256,128,1,1],f32>, !torch.none, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[16,256,64,32],f32>
  // TORCH-CHECK:       %permute_Y_val_0_conv_fprop = torch.constant.int 0
  // TORCH-CHECK:       %permute_Y_val_1_conv_fprop = torch.constant.int 1
  // TORCH-CHECK:       %permute_Y_val_2_conv_fprop = torch.constant.int 2
  // TORCH-CHECK:       %permute_Y_val_3_conv_fprop = torch.constant.int 3
  // TORCH-CHECK:       %permute_Y_conv_fprop = torch.prim.ListConstruct %permute_Y_val_0_conv_fprop, %permute_Y_val_1_conv_fprop, %permute_Y_val_2_conv_fprop, %permute_Y_val_3_conv_fprop : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
  // TORCH-CHECK:       %result = torch.aten.permute %result_perm, %permute_Y_conv_fprop : !torch.vtensor<[16,256,64,32],f32>, !torch.list<int> -> !torch.vtensor<[16,256,64,32],f32>
  // TORCH-CHECK:       torch.overwrite.tensor.contents %result overwrites %result_ : !torch.vtensor<[16,256,64,32],f32>, !torch.tensor<[16,256,64,32],f32>
  // TORCH-CHECK:       return
  // TORCH-CHECK:     }
  // TORCH-CHECK:   }
  //
  // LINALG-CHECK:  module @module {
  // LINALG-CHECK:    util.func public @main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.fence, %arg4: !hal.fence) attributes {inlining_policy = #util.inline.never, iree.abi.model = "coarse-fences", iree.abi.stub} {
  // LINALG-CHECK:      %cst = arith.constant 0.000000e+00 : f32
  // LINALG-CHECK:      %0 = hal.tensor.import wait(%arg3) => %arg1 : !hal.buffer_view -> tensor<16x128x64x32xf32>
  // LINALG-CHECK:      %1 = hal.tensor.import wait(%arg3) => %arg2 : !hal.buffer_view -> tensor<256x128x1x1xf32>
  // LINALG-CHECK:      %2 = tensor.empty() : tensor<16x256x64x32xf32>
  // LINALG-CHECK:      %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<16x256x64x32xf32>) -> tensor<16x256x64x32xf32>
  // LINALG-CHECK:      %4 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%0, %1 : tensor<16x128x64x32xf32>, tensor<256x128x1x1xf32>) outs(%3 : tensor<16x256x64x32xf32>) -> tensor<16x256x64x32xf32>
  // LINALG-CHECK:      %5 = hal.tensor.alias wait(%arg3) => %4 : tensor<16x256x64x32xf32> to %arg0 : !hal.buffer_view
  // LINALG-CHECK:      %6 = hal.tensor.barrier join(%5 : tensor<16x256x64x32xf32>) => %arg4 : !hal.fence
  // LINALG-CHECK:      util.return
  // LINALG-CHECK:    }
  // LINALG-CHECK:    util.func public @main(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) attributes {iree.abi.stub} {
  // LINALG-CHECK:      %0 = util.null : !hal.fence
  // LINALG-CHECK:      %c-1_i32 = arith.constant -1 : i32
  // LINALG-CHECK:      %c0 = arith.constant 0 : index
  // LINALG-CHECK:      %device_0 = hal.devices.get %c0 : !hal.device
  // LINALG-CHECK:      %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  // LINALG-CHECK:      util.call @main$async(%arg0, %arg1, %arg2, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> ()
  // LINALG-CHECK:      %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags("None") : i32
  // LINALG-CHECK:      util.return
  // LINALG-CHECK:    }
  // LINALG-CHECK:  }
  //
  // clang-format on

  assert(isOk(graph->validate()) && "Graph is invalid");
  ErrorOr<std::string> errorOrAsm = graph->emitAsm();
  assert(isOk(errorOrAsm) && "Graph ASM emission failed");
  std::cout << *errorOrAsm << std::endl;

  return 0;
}
